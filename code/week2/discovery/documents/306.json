{
  "asin": "1617291560", 
  "price": 42.49, 
  "reviewText": "I've had to hire recent graduates with degrees in machine learning, operations research and even &#34;data science.&#34; One of the problems with such people: they don't know anything practical. They probably know the basics of regression and some classification routines, as learned in their coursework. They've probably worked on one or many data science like problems, using machine learning techniques or regression or what not.  Many of them have never done a SQL query, or done the dirty business of data cleaning which takes up most of the data scientist's time. They'll always have gaps in their education; maybe they wrote a dissertation on an application of trees or deep learning, and have never used any of the other myriad tools available to the data scientist. None of them have ever done data science for money, and so none of them know about practical things like git or what the process looks like in an industrial setting. It is for these people that this book appears to be written. In an ideal world, all larval data scientists would be taught a course based on this book, or at least go through it themselves. It is also useful to experienced practitioners, as it covers many things, and can be a good practical reference to keep around.  The book is ordered as a data science project would be ordered, from start to finish; so, as you proceed down an engagement, reviewing the chapters in order will be helpful.Ch1 describes the job of the data scientist, the workflow, and the characters you run into on a project.Ch2 outlines some of the tools used to get at the data, including the authors tool, &#34;SQL Screwdriver.&#34; I'd have liked some genuflections at the unix tools used to clean data before it is put anywhere important; sed, awk, tr, sort and cut here, but I'm not sure if there is a graceful way of doing this. Or perhaps I'm the only weirdo who uses these in the ETL process.Ch3 exploring data; using the various plot utilities in ggplot2 (the graphics library everyone should be using); bar charts, histograms, summary statistics and scatter plots.Ch4 managing data: what they call &#34;cleaning data&#34; -I call reshaping data (and I use reshape, sometimes anyway; I would have mentioned this, though I got on well without it for years)Ch5 gets into specifying the problem; is it a classification problem? scoring? recommendation engine? How do I quantify success? This chapter is very helpful in doing this. Of course, problems evolve over time, and customers change their minds, but there are very helpful mappings here which will point you in the right direction There are a few new techniques which should probably be included in future editions of this chapter, depending on how they pan out: I'm impressed with using drop out techniques to prevent overfitting, for example (this is bleeding edge stuff, generally in context of deep learning).Ch6 Memorization techniques covers Naive Bayes, KNN and decision trees. It would have been nice to have more information on the various kinds of variable selection techniques (particularly important for NB and KNN), but mentioning this will allow the practitioner to go find their own information.Ch7 Logistic and Linear regression: most would have done these first, but these are actually more complex than memorization techniques, and there are more things to know to keep the practitioner out of trouble. In my opinion, this chapter really shines: everyone who is going to do this for a living has had some exposure to regression models: this chapter makes it practical.Ch8 Unsupervised methods; covers clustering; heirarchical clustering (one of the most useful tricks you will use in data science), kmeans (it has to be done, though I never found it to be useful) and association rules.Ch9 Advanced methods: GAMs, SVM, bagging and random forests (the importance measure trick: if you don't know it, pay attention: this is a very good trick). These are the &#34;industrial strength&#34; tools used in industry. I, personally would have stuck GAMs in their own chapter, and mentioned boosting here, but everyone is a little different in their tastes.Ch10 Documentation and deployment: they use Knitr; I just use vanilla Sweave (I've tried brew, but never took to it). They introduce git here: something I would have done in chapter 1 or 2, but it is a fairly natural place to mention it. They use the Rook tool to deploy HTTP services; I've never used it, though I have used Shiny, which I can recommend. They mention PMML briefly (I've never used it).The appendix on R is helpful, though it doesn't include the most valuable advice of all for using R in production: you need to maintain a distribution of R and all used packages, as well as a dependency toolchain if the code will be deployed on multiple servers.", 
  "title": "Practical Data Science with R"
}